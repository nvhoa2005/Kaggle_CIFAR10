{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3649,"databundleVersionId":46718,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-22T13:28:05.077746Z","iopub.execute_input":"2025-09-22T13:28:05.077987Z","iopub.status.idle":"2025-09-22T13:28:08.034399Z","shell.execute_reply.started":"2025-09-22T13:28:05.077969Z","shell.execute_reply":"2025-09-22T13:28:08.033617Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cifar-10/trainLabels.csv\n/kaggle/input/cifar-10/sampleSubmission.csv\n/kaggle/input/cifar-10/test.7z\n/kaggle/input/cifar-10/train.7z\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install py7zr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T13:28:08.035179Z","iopub.execute_input":"2025-09-22T13:28:08.035531Z","iopub.status.idle":"2025-09-22T13:28:15.945813Z","shell.execute_reply.started":"2025-09-22T13:28:08.035507Z","shell.execute_reply":"2025-09-22T13:28:15.945074Z"}},"outputs":[{"name":"stdout","text":"Collecting py7zr\n  Downloading py7zr-1.0.0-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: texttable in /usr/local/lib/python3.11/dist-packages (from py7zr) (1.7.0)\nRequirement already satisfied: pycryptodomex>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from py7zr) (3.23.0)\nCollecting brotli>=1.1.0 (from py7zr)\n  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from py7zr) (7.0.0)\nCollecting pyzstd>=0.16.1 (from py7zr)\n  Downloading pyzstd-0.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\nCollecting pyppmd<1.3.0,>=1.1.0 (from py7zr)\n  Downloading pyppmd-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\nCollecting pybcj<1.1.0,>=1.0.0 (from py7zr)\n  Downloading pybcj-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\nCollecting multivolumefile>=0.2.3 (from py7zr)\n  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\nCollecting inflate64<1.1.0,>=1.0.0 (from py7zr)\n  Downloading inflate64-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: typing-extensions>=4.13.2 in /usr/local/lib/python3.11/dist-packages (from pyzstd>=0.16.1->py7zr) (4.14.0)\nDownloading py7zr-1.0.0-py3-none-any.whl (69 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading inflate64-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\nDownloading pybcj-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyppmd-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.3/141.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyzstd-0.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.9/412.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: brotli, pyzstd, pyppmd, pybcj, multivolumefile, inflate64, py7zr\nSuccessfully installed brotli-1.1.0 inflate64-1.0.3 multivolumefile-0.2.3 py7zr-1.0.0 pybcj-1.0.6 pyppmd-1.2.0 pyzstd-0.17.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!mkdir -p pipeline models data submissions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T16:59:15.192224Z","iopub.execute_input":"2025-09-22T16:59:15.192435Z","iopub.status.idle":"2025-09-22T16:59:15.313381Z","shell.execute_reply.started":"2025-09-22T16:59:15.192405Z","shell.execute_reply":"2025-09-22T16:59:15.312375Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"%%writefile pipeline/const.py\nimport os\n\nBATCH_SIZE = 32\nNUM_WORKERS = os.cpu_count()\nCLASSES = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\nEPOCHS = 5\nLEARNING_RATE = 0.001\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T17:02:31.089945Z","iopub.execute_input":"2025-09-22T17:02:31.090241Z","iopub.status.idle":"2025-09-22T17:02:31.097286Z","shell.execute_reply.started":"2025-09-22T17:02:31.090214Z","shell.execute_reply":"2025-09-22T17:02:31.096703Z"}},"outputs":[{"name":"stdout","text":"Writing pipeline/const.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import py7zr\n# Đường dẫn đến các file cần giải nén\ntrain_file = '/kaggle/input/cifar-10/train.7z'\ntest_file = '/kaggle/input/cifar-10/test.7z'\n\n# Đường dẫn thư mục để giải nén\ntrain_output_dir = '/kaggle/working/data/train/'\ntest_output_dir = '/kaggle/working/data/test'\n\n# Giải nén train.7z\nwith py7zr.SevenZipFile(train_file, mode='r') as z:\n    z.extractall(path=train_output_dir)\nprint(\"Giải nén thành công train.7z\")\n\n# Giải nén test.7z\nwith py7zr.SevenZipFile(test_file, mode='r') as z:\n    z.extractall(path=test_output_dir)\nprint(\"Giải nén thành công test.7z\")\n\nprint(\"DONE!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T13:28:16.076948Z","iopub.execute_input":"2025-09-22T13:28:16.077209Z","iopub.status.idle":"2025-09-22T13:30:40.931565Z","shell.execute_reply.started":"2025-09-22T13:28:16.077185Z","shell.execute_reply":"2025-09-22T13:30:40.930824Z"}},"outputs":[{"name":"stdout","text":"Giải nén thành công train.7z\nGiải nén thành công test.7z\nDONE!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nkaggle_path = \"/kaggle/working\"\ndata_dir = os.path.join(kaggle_path, \"data\")\n\n# Đường dẫn thư mục\ntrain_dir = os.path.join(data_dir, \"train\", \"train\")\ntest_dir = os.path.join(data_dir, \"test\", \"test\")\n\n# Đếm số tệp trong thư mục train\nnum_train_files = len([f for f in os.listdir(train_dir) if os.path.isfile(os.path.join(train_dir, f))])\nprint(f\"Số tệp trong thư mục train: {num_train_files}\")\n\n# Đếm số tệp trong thư mục test\nnum_test_files = len([f for f in os.listdir(test_dir) if os.path.isfile(os.path.join(test_dir, f))])\nprint(f\"Số tệp trong thư mục test: {num_test_files}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T13:30:40.932525Z","iopub.execute_input":"2025-09-22T13:30:40.932962Z","iopub.status.idle":"2025-09-22T13:30:42.759634Z","shell.execute_reply.started":"2025-09-22T13:30:40.932940Z","shell.execute_reply":"2025-09-22T13:30:42.759057Z"}},"outputs":[{"name":"stdout","text":"Số tệp trong thư mục train: 50000\nSố tệp trong thư mục test: 300000\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"train_labels = pd.read_csv(\"/kaggle/input/cifar-10/trainLabels.csv\")\nsample_submissions = pd.read_csv(\"/kaggle/input/cifar-10/sampleSubmission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T13:30:42.760501Z","iopub.execute_input":"2025-09-22T13:30:42.760830Z","iopub.status.idle":"2025-09-22T13:30:42.900416Z","shell.execute_reply.started":"2025-09-22T13:30:42.760810Z","shell.execute_reply":"2025-09-22T13:30:42.899807Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"%%writefile pipeline/data_setup.py\nimport os\nimport torch\nimport pathlib\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport pandas as pd\nfrom PIL import Image\nfrom typing import Tuple\n\nfrom pipeline import const\n\nimport importlib\nimportlib.reload(const)\n\nNUM_WORKERS = const.NUM_WORKERS\nBATCH_SIZE = const.BATCH_SIZE\nCLASSES = const.CLASSES\n\n# Write a custom dataset class (inherits from torch.utils.data.Dataset)\nclass_to_idx = {cls_name: i for i, cls_name in enumerate(CLASSES)}\n# 1. Subclass torch.utils.data.Dataset\nclass TrainDatasetCustom(Dataset):\n    \n    # 2. Initialize with a targ_dir and transform (optional) parameter\n    def __init__(self, targ_dir: str, labels: pd.DataFrame, transform=None) -> None:\n        \n        # 3. Create class attributes\n        # Get all .png images and sort them by filename (numerically)\n        self.paths = sorted(pathlib.Path(targ_dir).glob(\"*.png\"), key=lambda x: int(x.stem))\n        # Setup transforms\n        self.transform = transform\n        # Create classes and class_to_idx attributes\n        self.classes, self.class_to_idx = CLASSES, class_to_idx\n        # Set up labels\n        self.labels = labels\n\n    # 4. Make function to load images\n    def load_image(self, index: int) -> Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = self.paths[index]\n        return Image.open(image_path) \n    \n    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n    def __len__(self) -> int:\n        \"Returns the total number of samples.\"\n        return len(self.paths)\n    \n    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n        \"Returns one sample of data, data and label (X, y).\"\n        img = self.load_image(index)\n        class_name  = self.labels[\"label\"][index]\n        class_idx = self.class_to_idx[class_name]\n\n        # Transform if necessary\n        if self.transform:\n            return self.transform(img), class_idx # return data, label (X, y)\n        else:\n            return img, class_idx # return data, label (X, y)\n\n# 1. Subclass torch.utils.data.Dataset\nclass TestDatasetCustom(Dataset):\n    \n    # 2. Initialize with a targ_dir and transform (optional) parameter\n    def __init__(self, targ_dir: str, labels: pd.DataFrame, transform=None) -> None:\n        \n        # 3. Create class attributes\n        # Get all .png images and sort them by filename (numerically)\n        self.paths = sorted(pathlib.Path(targ_dir).glob(\"*.png\"), key=lambda x: int(x.stem))\n        # Setup transforms\n        self.transform = transform\n        # Create classes and class_to_idx attributes\n        self.classes, self.class_to_idx = CLASSES, class_to_idx\n        # Set up labels\n        self.labels = labels\n\n    # 4. Make function to load images\n    def load_image(self, index: int) -> Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = self.paths[index]\n        return Image.open(image_path) \n    \n    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n    def __len__(self) -> int:\n        \"Returns the total number of samples.\"\n        return len(self.paths)\n    \n    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n        image_id = int(self.labels.iloc[index]['id'])\n        image_path = self.paths[index]\n        img = Image.open(image_path).convert(\"RGB\")\n\n        if self.transform:\n            img = self.transform(img)\n\n        return img, image_id  # trả về ảnh và id\n\n\n\ndef create_dataloaders(\n    train_dir: str, \n    test_dir: str, \n    train_transform: transforms.Compose, \n    test_transform: transforms.Compose,\n    train_labels: pd.DataFrame,\n    test_labels: pd.DataFrame,\n    batch_size: int, \n    num_workers: int=NUM_WORKERS\n):\n    \n    # Use TrainDatasetCustom to create dataset(s)\n    train_data_custom = TrainDatasetCustom(targ_dir=train_dir,\n                                      labels=train_labels,\n                                      transform=train_transform)\n    test_data_custom = TestDatasetCustom(targ_dir=test_dir,\n                                      labels=test_labels,\n                                      transform=test_transform)\n    \n    # Get class names\n    class_names = train_data_custom.classes\n    \n    # Turn images into data loaders\n    train_dataloader = DataLoader(\n      train_data_custom,\n      batch_size=batch_size,\n      shuffle=True,\n      num_workers=num_workers,\n      pin_memory=True,\n    )\n    \n    test_dataloader = DataLoader(\n      test_data_custom,\n      batch_size=batch_size,\n      shuffle=False, # don't need to shuffle test data\n      num_workers=num_workers,\n      pin_memory=True,\n    )\n\n    return train_dataloader, test_dataloader, class_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T17:02:34.672146Z","iopub.execute_input":"2025-09-22T17:02:34.672664Z","iopub.status.idle":"2025-09-22T17:02:34.679687Z","shell.execute_reply.started":"2025-09-22T17:02:34.672632Z","shell.execute_reply":"2025-09-22T17:02:34.678962Z"}},"outputs":[{"name":"stdout","text":"Writing pipeline/data_setup.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Import data_setup.py\nfrom pipeline import data_setup\nfrom torchvision import transforms\nfrom pipeline import const\n\nimport importlib\nimportlib.reload(data_setup)  # ép Python load lại code mới từ file\n\nBATCH_SIZE = const.BATCH_SIZE\nNUM_WORKERS = const.NUM_WORKERS\n\n\n# Augment train data\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ToTensor()\n])\n\n# Don't augment test data, only reshape\ntest_transform = transforms.Compose([\n    transforms.ToTensor()\n])\n\n# Create train/test dataloader and get class names as a list\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                               test_dir=test_dir,\n                                                                               train_transform=train_transform,\n                                                                               test_transform=test_transform,\n                                                                               train_labels=train_labels,\n                                                                               test_labels=sample_submissions,\n                                                                               batch_size=BATCH_SIZE,\n                                                                               num_workers=NUM_WORKERS)\ntrain_dataloader, test_dataloader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:31:46.291521Z","iopub.execute_input":"2025-09-22T14:31:46.291828Z","iopub.status.idle":"2025-09-22T14:32:09.860592Z","shell.execute_reply.started":"2025-09-22T14:31:46.291809Z","shell.execute_reply":"2025-09-22T14:32:09.860030Z"}},"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"(<torch.utils.data.dataloader.DataLoader at 0x7a02252f0690>,\n <torch.utils.data.dataloader.DataLoader at 0x7a0229adbbd0>)"},"metadata":{}}],"execution_count":84},{"cell_type":"code","source":"%%writefile pipeline/model_builder.py\n\"\"\"\nContains PyTorch model code to instantiate an EfficientNetB0 model.\n\"\"\"\nimport torch\nfrom torch import nn\nimport torchvision.models as models\n\nclass EfficientNetB0(nn.Module):\n    \"\"\"Creates an EfficientNetB0 architecture for transfer learning.\n\n    Args:\n        output_shape: An integer indicating number of output classes.\n        pretrained: Whether to use pretrained weights on ImageNet.\n    \"\"\"\n    def __init__(self, in_features: int, output_shape: int, pretrained: bool = True) -> None:\n        super().__init__()\n        # Load EfficientNetB0 pretrained on ImageNet1K\n        weights = models.EfficientNet_B0_Weights.DEFAULT if pretrained else None\n        self.model = models.efficientnet_b0(weights=weights)\n\n        # Mở toàn bộ trọng số (unfreeze) để fine-tune full model\n        for param in self.model.parameters():\n            param.requires_grad = True\n\n        # Thay classifier cuối cùng bằng lớp Linear mới\n        self.model.classifier = torch.nn.Sequential(\n            torch.nn.Dropout(p=0.2, inplace=True), \n            torch.nn.Linear(in_features=in_features, \n                            out_features=output_shape, # same number of output units as our number of classes\n                            bias=True))\n\n    def forward(self, x: torch.Tensor):\n        return self.model(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T17:02:39.523949Z","iopub.execute_input":"2025-09-22T17:02:39.524654Z","iopub.status.idle":"2025-09-22T17:02:39.529697Z","shell.execute_reply.started":"2025-09-22T17:02:39.524628Z","shell.execute_reply":"2025-09-22T17:02:39.529161Z"}},"outputs":[{"name":"stdout","text":"Writing pipeline/model_builder.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\n# Import model_builder.py\nfrom pipeline import model_builder\nfrom pipeline import const\nimportlib.reload(model_builder)\nCLASSES = const.CLASSES\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Instantiate an instance of the model from the \"model_builder.py\" script\ntorch.manual_seed(42)\nmodel = model_builder.EfficientNetB0(in_features=1280,\n                                        pretrained = True,\n                                          output_shape=len(CLASSES)).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T16:10:28.291157Z","iopub.execute_input":"2025-09-22T16:10:28.291477Z","iopub.status.idle":"2025-09-22T16:10:28.441808Z","shell.execute_reply.started":"2025-09-22T16:10:28.291457Z","shell.execute_reply":"2025-09-22T16:10:28.441053Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"%%writefile pipeline/engine.py\n\"\"\"\nContains functions for training and testing a PyTorch model.\n\"\"\"\nimport torch\n\nfrom tqdm.auto import tqdm\nfrom typing import Dict, List, Tuple\n\ndef train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer,\n               device: torch.device) -> Tuple[float, float]:\n    # Put model in train mode\n    model.train()\n    \n    # Setup train loss and train accuracy values\n    train_loss, train_acc = 0, 0\n    \n    # Loop through data loader data batches\n    for batch, (X, y) in enumerate(dataloader):\n        # Send data to target device\n        X, y = X.to(device), y.to(device)\n\n        # 1. Forward pass\n        y_pred = model(X)\n\n        # 2. Calculate  and accumulate loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss.item() \n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n        # Calculate and accumulate accuracy metrics across all batches\n        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n    # Adjust metrics to get average loss and accuracy per batch \n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc\n\n# 1. Take in various parameters required for training and test steps\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          device: torch.device,\n          loss_fn: torch.nn.Module = torch.nn.CrossEntropyLoss(),\n          epochs: int = 5) -> Dict[str, List]:\n    \n    # 2. Create empty results dictionary\n    results = {\"train_loss\": [],\n        \"train_acc\": []\n    }\n    \n    # 3. Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                           device=device,\n                                           dataloader=train_dataloader,\n                                           loss_fn=loss_fn,\n                                           optimizer=optimizer)\n        \n        # 4. Print out what's happening\n        print(\n            f\"Epoch: {epoch+1} | \"\n            f\"train_loss: {train_loss:.4f} | \"\n            f\"train_acc: {train_acc:.4f} | \"\n        )\n\n        # 5. Update results dictionary\n        # Ensure all data is moved to CPU and converted to float for storage\n        results[\"train_loss\"].append(train_loss.item() if isinstance(train_loss, torch.Tensor) else train_loss)\n        results[\"train_acc\"].append(train_acc.item() if isinstance(train_acc, torch.Tensor) else train_acc)\n\n    # 6. Return the filled results at the end of the epochs\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T17:02:42.030880Z","iopub.execute_input":"2025-09-22T17:02:42.031557Z","iopub.status.idle":"2025-09-22T17:02:42.036996Z","shell.execute_reply.started":"2025-09-22T17:02:42.031531Z","shell.execute_reply":"2025-09-22T17:02:42.036197Z"}},"outputs":[{"name":"stdout","text":"Writing pipeline/engine.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from pipeline import engine\nfrom pipeline import const\nfrom torch import nn\n\nimport importlib\nimportlib.reload(const)\nimportlib.reload(engine)\n\nEPOCHS = const.EPOCHS\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n\n# Use train() by calling it from engine.py\nengine.train(model=model,\n             train_dataloader=train_dataloader,\n             optimizer=optimizer,\n             loss_fn=loss_fn,\n             epochs=EPOCHS,\n             device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:32:23.364214Z","iopub.execute_input":"2025-09-22T14:32:23.364792Z","iopub.status.idle":"2025-09-22T14:33:13.726984Z","shell.execute_reply.started":"2025-09-22T14:32:23.364770Z","shell.execute_reply":"2025-09-22T14:33:13.726274Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d426b6c09a84c78b2ffe390eeb59783"}},"metadata":{}},{"name":"stdout","text":"Epoch: 1 | train_loss: 1.9285 | train_acc: 0.2957 | \nEpoch: 2 | train_loss: 1.6025 | train_acc: 0.4217 | \nEpoch: 3 | train_loss: 1.4946 | train_acc: 0.4624 | \nEpoch: 4 | train_loss: 1.4463 | train_acc: 0.4801 | \nEpoch: 5 | train_loss: 1.4219 | train_acc: 0.4916 | \n","output_type":"stream"},{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"{'train_loss': [1.9285157425466135,\n  1.6025101368959638,\n  1.4946407941542446,\n  1.4463360762260544,\n  1.421942500563211],\n 'train_acc': [0.295685380678183,\n  0.42174504158669224,\n  0.46243202175303905,\n  0.4800663787587972,\n  0.49164267434420983]}"},"metadata":{}}],"execution_count":88},{"cell_type":"code","source":"%%writefile pipeline/utils.py\n\"\"\"\nContains various utility functions for PyTorch model training and saving.\n\"\"\"\nimport torch\nfrom pathlib import Path\n\ndef save_model(model: torch.nn.Module,\n               target_dir: str,\n               model_name: str):\n  \"\"\"Saves a PyTorch model to a target directory.\n\n  Args:\n    model: A target PyTorch model to save.\n    target_dir: A directory for saving the model to.\n    model_name: A filename for the saved model. Should include\n      either \".pth\" or \".pt\" as the file extension.\n  \n  Example usage:\n    save_model(model=model_0,\n               target_dir=\"models\",\n               model_name=\"05_going_modular_tingvgg_model.pth\")\n  \"\"\"\n  # Create target directory\n  target_dir_path = Path(target_dir)\n  target_dir_path.mkdir(parents=True,\n                        exist_ok=True)\n  \n  # Create model save path\n  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n  model_save_path = target_dir_path / model_name\n\n  # Save the model state_dict()\n  print(f\"[INFO] Saving model to: {model_save_path}\")\n  torch.save(obj=model.state_dict(),\n             f=model_save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T17:02:46.570193Z","iopub.execute_input":"2025-09-22T17:02:46.570440Z","iopub.status.idle":"2025-09-22T17:02:46.575282Z","shell.execute_reply.started":"2025-09-22T17:02:46.570419Z","shell.execute_reply":"2025-09-22T17:02:46.574514Z"}},"outputs":[{"name":"stdout","text":"Writing pipeline/utils.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Import utils.py\nfrom pipeline import utils\n\n# Save a model to file\nutils.save_model(model=model,\n           target_dir=\"models\",\n           model_name=\"example_model.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:33:13.735417Z","iopub.execute_input":"2025-09-22T14:33:13.735615Z","iopub.status.idle":"2025-09-22T14:33:13.764456Z","shell.execute_reply.started":"2025-09-22T14:33:13.735599Z","shell.execute_reply":"2025-09-22T14:33:13.763587Z"}},"outputs":[{"name":"stdout","text":"[INFO] Saving model to: models/example_model.pth\n","output_type":"stream"}],"execution_count":90},{"cell_type":"code","source":"from pipeline import model_builder\n\nloaded_model = model_builder.TinyVGG(\n    input_shape=3,\n    hidden_units=10,\n    output_shape=len(CLASSES)\n).to(device)\n\n# Load trọng số\nmodel_path = \"models/example_model.pth\"\nloaded_model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:33:58.274725Z","iopub.execute_input":"2025-09-22T14:33:58.275043Z","iopub.status.idle":"2025-09-22T14:33:58.286175Z","shell.execute_reply.started":"2025-09-22T14:33:58.274987Z","shell.execute_reply":"2025-09-22T14:33:58.285376Z"}},"outputs":[{"execution_count":94,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":94},{"cell_type":"code","source":"%%writefile pipeline/predict.py\nfrom pipeline import model_builder\nimport torch\nimport pandas as pd\n\nfrom pipeline import const\nCLASSES = const.CLASSES\nclass_to_idx = {cls_name: i for i, cls_name in enumerate(CLASSES)}\nidx_to_class = {v: k for k, v in class_to_idx.items()}\n\ndef predict_testset(model: torch.nn.Module,\n                    submission_number: int,\n                    submission_dir: str,\n                    test_dataloader: torch.utils.data.DataLoader,\n                    device: torch.device):\n    submission_data = []\n    \n    model.eval()\n    with torch.inference_mode():\n        for imgs, image_ids in test_dataloader:\n            outputs = model(imgs.to(device))\n            preds = torch.argmax(torch.softmax(outputs, dim=1), dim=1)\n    \n            for img_id, pred in zip(image_ids, preds):\n                predicted_label = idx_to_class[pred.item()]\n                submission_data.append({'id': int(img_id), 'label': predicted_label})\n                if(int(img_id) % 10000 == 0):\n                    print(f\"Done ảnh {img_id}\")\n                    \n    # Convert the submission data to a DataFrame\n    submission_df = pd.DataFrame(submission_data)\n    \n    # Save the submission file\n    submission_df.to_csv(f'{submission_dir}/submission{submission_number}.csv', index=False)\n    \n    print(f\"Submission file saved to 'submission{submission_number}.csv'.\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T17:02:49.976469Z","iopub.execute_input":"2025-09-22T17:02:49.976724Z","iopub.status.idle":"2025-09-22T17:02:49.982148Z","shell.execute_reply.started":"2025-09-22T17:02:49.976706Z","shell.execute_reply":"2025-09-22T17:02:49.981487Z"}},"outputs":[{"name":"stdout","text":"Writing pipeline/predict.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from pipeline import predict\nimport importlib\nimportlib.reload(predict)\n\nsubmission_dir = os.path.join(kaggle_path, \"submissions\")\n\npredict.predict_testset(model=loaded_model,\n                submission_number=1,\n                submission_dir=submission_dir,\n                test_dataloader=test_dataloader,\n                device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:35:32.141827Z","iopub.execute_input":"2025-09-22T14:35:32.142141Z","iopub.status.idle":"2025-09-22T14:36:27.533261Z","shell.execute_reply.started":"2025-09-22T14:35:32.142117Z","shell.execute_reply":"2025-09-22T14:36:27.532338Z"}},"outputs":[{"name":"stdout","text":"Done ảnh 10000\nDone ảnh 20000\nDone ảnh 30000\nDone ảnh 40000\nDone ảnh 50000\nDone ảnh 60000\nDone ảnh 70000\nDone ảnh 80000\nDone ảnh 90000\nDone ảnh 100000\nDone ảnh 110000\nDone ảnh 120000\nDone ảnh 130000\nDone ảnh 140000\nDone ảnh 150000\nDone ảnh 160000\nDone ảnh 170000\nDone ảnh 180000\nDone ảnh 190000\nDone ảnh 200000\nDone ảnh 210000\nDone ảnh 220000\nDone ảnh 230000\nDone ảnh 240000\nDone ảnh 250000\nDone ảnh 260000\nDone ảnh 270000\nDone ảnh 280000\nDone ảnh 290000\nDone ảnh 300000\nSubmission file saved to 'submission1.csv'.\n","output_type":"stream"}],"execution_count":98},{"cell_type":"code","source":"%%writefile pipeline/train.py\n\"\"\"\nTrains a PyTorch image classification model using device-agnostic code.\n\"\"\"\n\nimport os\nimport torch\nfrom pipeline import data_setup, engine, model_builder, utils, const, predict\nimport pandas as pd\n\nfrom torchvision import transforms\n\nimport importlib\nimportlib.reload(data_setup)\nimportlib.reload(engine)\nimportlib.reload(model_builder)\nimportlib.reload(utils)\nimportlib.reload(const)\nimportlib.reload(predict)\n\n# Setup hyperparameters\nEPOCHS = const.EPOCHS\nBATCH_SIZE = const.BATCH_SIZE\nCLASSES = const.CLASSES\nHIDDEN_UNITS = len(CLASSES)\nLEARNING_RATE = const.LEARNING_RATE\nNUM_WORKERS = const.NUM_WORKERS\n\n# Setup directories\nkaggle_path = \"/kaggle/working\"\ndata_dir = os.path.join(kaggle_path, \"data\")\n\n# Đường dẫn thư mục\ntrain_dir = os.path.join(data_dir, \"train\", \"train\")\ntest_dir = os.path.join(data_dir, \"test\", \"test\")\n\n# Setup target device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# DataFrame\ntrain_labels = pd.read_csv(\"/kaggle/input/cifar-10/trainLabels.csv\")\nsample_submissions = pd.read_csv(\"/kaggle/input/cifar-10/sampleSubmission.csv\")\n\n# Create transform\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n    transforms.ToTensor(), # 2. Turn image values to between 0 & 1 \n    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n])\ntest_transform = transforms.Compose([\n    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n    transforms.ToTensor(), # 2. Turn image values to between 0 & 1 \n    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n])\n\n# Create train/test dataloader and get class names as a list\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                               test_dir=test_dir,\n                                                                               train_transform=train_transform,\n                                                                               test_transform=test_transform,\n                                                                               train_labels=train_labels,\n                                                                               test_labels=sample_submissions,\n                                                                               batch_size=BATCH_SIZE,\n                                                                               num_workers=NUM_WORKERS)\n\n# Create model with help from model_builder.py\nmodel = model_builder.EfficientNetB0(in_features=1280,\n                                        pretrained = True,\n                                          output_shape=len(CLASSES)).to(device)\n\n# Set loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),\n                             lr=LEARNING_RATE)\n\n# Start training with help from engine.py\nengine.train(model=model,\n             train_dataloader=train_dataloader,\n             optimizer=optimizer,\n             loss_fn=loss_fn,\n             epochs=EPOCHS,\n             device=device)\n\n# Save the model with help from utils.py\nutils.save_model(model=model,\n           target_dir=os.path.join(kaggle_path, \"models\"),\n           model_name=\"example_model.pth\")\n\nloaded_model = model_builder.EfficientNetB0(\n    in_features=1280,\n    pretrained = True,\n    output_shape=len(CLASSES)).to(device)         \n\n# Load trọng số\nmodel_path = os.path.join(kaggle_path, \"models\", \"example_model.pth\")\nloaded_model.load_state_dict(torch.load(model_path, map_location=device))\n\n# Predict và lưu vào file submission\nsubmission_dir = os.path.join(kaggle_path, \"submissions\")\n\npredict.predict_testset(model=loaded_model,\n                        submission_number=1,\n                        submission_dir=submission_dir,\n                        test_dataloader=test_dataloader,\n                        device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T17:02:53.938459Z","iopub.execute_input":"2025-09-22T17:02:53.938744Z","iopub.status.idle":"2025-09-22T17:02:53.945292Z","shell.execute_reply.started":"2025-09-22T17:02:53.938722Z","shell.execute_reply":"2025-09-22T17:02:53.944540Z"}},"outputs":[{"name":"stdout","text":"Writing pipeline/train.py\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"%%writefile pipeline/main.py\n\nimport os\nimport py7zr\nimport torch\n\ndef main():\n    # 1. Cài thư mục cần thiết\n    os.makedirs(\"pipeline\", exist_ok=True)\n    os.makedirs(\"models\", exist_ok=True)\n    os.makedirs(\"data\", exist_ok=True)\n    os.makedirs(\"submissions\", exist_ok=True)\n    \n    # 2. Đường dẫn đến file .7z\n    train_file = '/kaggle/input/cifar-10/train.7z'\n    test_file  = '/kaggle/input/cifar-10/test.7z'\n    \n    train_output_dir = '/kaggle/working/data/train/'\n    test_output_dir  = '/kaggle/working/data/test/'\n    \n    # 3. Giải nén dữ liệu train\n    with py7zr.SevenZipFile(train_file, mode='r') as z:\n        z.extractall(path=train_output_dir)\n    print(\"[INFO] Giải nén thành công train.7z\")\n    \n    # 4. Giải nén dữ liệu test\n    with py7zr.SevenZipFile(test_file, mode='r') as z:\n        z.extractall(path=test_output_dir)\n    print(\"[INFO] Giải nén thành công test.7z\")\n    \n    print(\"[INFO] DONE extract!\")\n    \n    # 5. Gọi script train.py trong pipeline\n    print(\"[INFO] Bắt đầu train...\")\n    os.system(\"python -m pipeline.train\")\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T17:03:32.645080Z","iopub.execute_input":"2025-09-22T17:03:32.645343Z","iopub.status.idle":"2025-09-22T17:03:32.650410Z","shell.execute_reply.started":"2025-09-22T17:03:32.645323Z","shell.execute_reply":"2025-09-22T17:03:32.649730Z"}},"outputs":[{"name":"stdout","text":"Overwriting pipeline/main.py\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"### CHẠY TẤT CẢ CÁC CELL TẠO FILE VÀ CHẠY CÁC DÒNG CODE SAU","metadata":{}},{"cell_type":"code","source":"!touch pipeline/__init__.py\n!pip install py7zr\n!python pipeline/main.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T17:04:00.046486Z","iopub.execute_input":"2025-09-22T17:04:00.047029Z","iopub.status.idle":"2025-09-22T17:33:22.157168Z","shell.execute_reply.started":"2025-09-22T17:04:00.047004Z","shell.execute_reply":"2025-09-22T17:33:22.156251Z"}},"outputs":[{"name":"stdout","text":"Collecting py7zr\n  Downloading py7zr-1.0.0-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: texttable in /usr/local/lib/python3.11/dist-packages (from py7zr) (1.7.0)\nRequirement already satisfied: pycryptodomex>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from py7zr) (3.23.0)\nCollecting brotli>=1.1.0 (from py7zr)\n  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from py7zr) (7.0.0)\nCollecting pyzstd>=0.16.1 (from py7zr)\n  Downloading pyzstd-0.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\nCollecting pyppmd<1.3.0,>=1.1.0 (from py7zr)\n  Downloading pyppmd-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\nCollecting pybcj<1.1.0,>=1.0.0 (from py7zr)\n  Downloading pybcj-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\nCollecting multivolumefile>=0.2.3 (from py7zr)\n  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\nCollecting inflate64<1.1.0,>=1.0.0 (from py7zr)\n  Downloading inflate64-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: typing-extensions>=4.13.2 in /usr/local/lib/python3.11/dist-packages (from pyzstd>=0.16.1->py7zr) (4.14.0)\nDownloading py7zr-1.0.0-py3-none-any.whl (69 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading inflate64-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\nDownloading pybcj-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyppmd-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.3/141.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyzstd-0.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.9/412.9 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: brotli, pyzstd, pyppmd, pybcj, multivolumefile, inflate64, py7zr\nSuccessfully installed brotli-1.1.0 inflate64-1.0.3 multivolumefile-0.2.3 py7zr-1.0.0 pybcj-1.0.6 pyppmd-1.2.0 pyzstd-0.17.0\n[INFO] Giải nén thành công train.7z\n[INFO] Giải nén thành công test.7z\n[INFO] DONE extract!\n[INFO] Bắt đầu train...\nDownloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n100%|███████████████████████████████████████| 20.5M/20.5M [00:00<00:00, 119MB/s]\n  0%|                                                     | 0/5 [00:00<?, ?it/s]Epoch: 1 | train_loss: 0.4513 | train_acc: 0.8480 | \n 20%|████████▊                                   | 1/5 [04:00<16:02, 240.51s/it]Epoch: 2 | train_loss: 0.2663 | train_acc: 0.9101 | \n 40%|█████████████████▌                          | 2/5 [08:06<12:10, 243.65s/it]Epoch: 3 | train_loss: 0.2089 | train_acc: 0.9293 | \n 60%|██████████████████████████▍                 | 3/5 [12:12<08:09, 244.88s/it]Epoch: 4 | train_loss: 0.1716 | train_acc: 0.9413 | \n 80%|███████████████████████████████████▏        | 4/5 [16:18<04:05, 245.39s/it]Epoch: 5 | train_loss: 0.1452 | train_acc: 0.9508 | \n100%|████████████████████████████████████████████| 5/5 [20:24<00:00, 244.99s/it]\n[INFO] Saving model to: /kaggle/working/models/example_model.pth\nDone ảnh 10000\nDone ảnh 20000\nDone ảnh 30000\nDone ảnh 40000\nDone ảnh 50000\nDone ảnh 60000\nDone ảnh 70000\nDone ảnh 80000\nDone ảnh 90000\nDone ảnh 100000\nDone ảnh 110000\nDone ảnh 120000\nDone ảnh 130000\nDone ảnh 140000\nDone ảnh 150000\nDone ảnh 160000\nDone ảnh 170000\nDone ảnh 180000\nDone ảnh 190000\nDone ảnh 200000\nDone ảnh 210000\nDone ảnh 220000\nDone ảnh 230000\nDone ảnh 240000\nDone ảnh 250000\nDone ảnh 260000\nDone ảnh 270000\nDone ảnh 280000\nDone ảnh 290000\nDone ảnh 300000\nSubmission file saved to 'submission1.csv'.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}